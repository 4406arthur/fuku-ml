{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron Binary Classification Learning Algorithm Tutorial\n",
    "\n",
    "註：這個 Tutorial 主要還是介紹怎麼使用 FukuML，如果非必要並不會涉入太多演算法或數學式的細節，若大家對機器學習有興趣，還是建議觀看完整的課程。\n",
    "\n",
    "Perceptron Binary Classification Learning Algorithm（PLA）是最基礎的機器學習算法，其核心想法也不難，追根究底就是個知錯能改的演算法，只要有錯就修正分類器，直到不會犯錯為止。PLA 也是一個最基礎的類神經網路的運算神經元，現在很紅的 Deep Learning 的最基礎概念其實就是 PLA，因此了解 PLA 對未來學習機器學習這門課程是很有幫助的。\n",
    "\n",
    "底下列出幾個 PLA 相關的數學式，方便大家日後學習時查閱：\n",
    "\n",
    "### PLA 假設\n",
    "\n",
    "$$\n",
    "h(x) = sign(w^Tx)\n",
    "$$\n",
    "\n",
    "表示 PLA 對資料每一個維度的權重假設，這個權重向量在式子中以 w 表示，所以我們利用 PLA 學習出最能夠分好類的 w 之後，將 x 丟進去這個 PLA 假設，它就會告訴你分類的結果。\n",
    "\n",
    "### PLA 犯錯 \n",
    "\n",
    "$$\n",
    "sign(w_t^Tx_{n(t)}) \\neq y_{n(t)}\n",
    "$$\n",
    "\n",
    "表示 PLA 對哪個資料點是預測錯誤的，其實就是對目前的假設 $w_t$ 對 $x_{n(t)}$ 點進行內積再取正負號，如果與 $y_{n(t)}$ 不同，那就代表 PLA 犯錯了。 \n",
    "\n",
    "### PLA 修正假設\n",
    "\n",
    "$$\n",
    "w_{t+1} = w_t + y_{n(t)}x_{n(t)}\n",
    "$$\n",
    "\n",
    "表示 PLA 犯錯之後怎麼修正，如果 PLA 猜 +1 但答案是 -1，那就往 $-1(x_{n(t)})$ 對 $w_t$ 做修正；如果 PLA 猜 -1 但答案是 +1，那就往 $+1(x_{n(t)})$ 對 $w_t$ 做修正。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用 FukuML 的 PLA 做二元分類\n",
    "\n",
    "接下來讓我們一步一步學習如何使用 FukuML 的 PLA 來做二元分類，首先讓我們將 PLA 引進來："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import FukuML.PLA as pla"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然後建構一個 PLA 二元分類物件："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pla_bc = pla.BinaryClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我希望 FukuML 能儘量簡單易用，因此大家只要牢記 1. 載入訓練資料 -> 2. 設定參數 -> 3. 初始化 -> 4. 訓練 -> 5. 預測 這五個步驟就可以完成機器學習了～\n",
    "\n",
    "現在第一個步驟要先載入訓練資料，但如果現在要讓大家生出一筆訓練資料應該會有困難，所以 FukuML 每個機器學習演算法都會有一個 Demo 用的內建資料，讓我們先用 Demo 用的內建資料來試試看。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 1.      ,  0.97681 ,  0.10723 ,  0.64385 ,  0.29556 ],\n",
       "        [ 1.      ,  0.67194 ,  0.2418  ,  0.83075 ,  0.42741 ],\n",
       "        [ 1.      ,  0.20619 ,  0.23321 ,  0.81004 ,  0.98691 ],\n",
       "        ..., \n",
       "        [ 1.      ,  0.50468 ,  0.99699 ,  0.75136 ,  0.51681 ],\n",
       "        [ 1.      ,  0.55852 ,  0.067689,  0.666   ,  0.98482 ],\n",
       "        [ 1.      ,  0.83188 ,  0.66817 ,  0.23403 ,  0.72472 ]]),\n",
       " array([ 1.,  1.,  1.,  1.,  1.,  1., -1.,  1., -1., -1.,  1.,  1.,  1.,\n",
       "        -1., -1.,  1.,  1.,  1., -1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "        -1.,  1.,  1., -1., -1.,  1.,  1., -1.,  1.,  1., -1., -1.,  1.,\n",
       "        -1., -1.,  1., -1.,  1.,  1.,  1., -1., -1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1., -1., -1.,  1., -1.,  1., -1., -1.,  1.,\n",
       "        -1.,  1., -1., -1.,  1.,  1.,  1., -1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1., -1.,  1.,  1.,  1., -1.,  1.,  1., -1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1., -1.,  1., -1.,  1.,  1., -1.,  1.,  1.,  1.,  1.,\n",
       "        -1.,  1.,  1.,  1.,  1., -1.,  1., -1.,  1.,  1., -1.,  1.,  1.,\n",
       "         1.,  1., -1.,  1., -1., -1., -1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1., -1., -1.,  1.,  1., -1.,  1., -1.,  1.,  1.,  1., -1.,  1.,\n",
       "        -1., -1.,  1., -1., -1.,  1.,  1.,  1.,  1., -1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1., -1., -1., -1.,  1., -1.,  1., -1.,  1.,\n",
       "        -1.,  1.,  1., -1., -1.,  1., -1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1., -1.,  1.,  1., -1.,  1.,  1.,  1.,  1.,  1., -1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1., -1., -1., -1., -1.,  1., -1.,  1.,  1.,\n",
       "        -1.,  1., -1., -1.,  1.,  1.,  1.,  1.,  1.,  1.,  1., -1.,  1.,\n",
       "        -1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1., -1.,\n",
       "         1., -1.,  1.,  1., -1.,  1.,  1.,  1.,  1., -1.,  1., -1.,  1.,\n",
       "         1.,  1.,  1.,  1., -1.,  1., -1.,  1.,  1.,  1., -1., -1.,  1.,\n",
       "         1.,  1.,  1.,  1., -1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1., -1.,  1.,  1.,  1.,  1., -1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1., -1.,  1., -1.,  1.,  1.,  1.,\n",
       "        -1., -1., -1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1., -1.,\n",
       "         1.,  1.,  1., -1.,  1.,  1., -1., -1., -1.,  1.,  1., -1., -1.,\n",
       "         1., -1., -1., -1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1., -1.,\n",
       "         1.,  1.,  1.,  1., -1.,  1.,  1., -1., -1.,  1., -1.,  1.,  1.,\n",
       "        -1.,  1.,  1.,  1.,  1.,  1., -1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1., -1.,  1.,  1.,  1., -1.,  1., -1.,  1.,  1.,  1., -1.,\n",
       "         1.,  1.,  1., -1., -1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pla_bc.load_train_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "這樣就載入了 PLA 的 Demo 訓練資料，不信的話大家可以使用 `pla_bc.train_X` 及 `pla_bc.train_Y` 印出來看看："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.        0.97681   0.10723   0.64385   0.29556 ]\n",
      " [ 1.        0.67194   0.2418    0.83075   0.42741 ]\n",
      " [ 1.        0.20619   0.23321   0.81004   0.98691 ]\n",
      " ..., \n",
      " [ 1.        0.50468   0.99699   0.75136   0.51681 ]\n",
      " [ 1.        0.55852   0.067689  0.666     0.98482 ]\n",
      " [ 1.        0.83188   0.66817   0.23403   0.72472 ]]\n"
     ]
    }
   ],
   "source": [
    "print(pla_bc.train_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "訓練資料的特徵資料就存在 `train_X` 中，矩陣的每一個列就代表一筆資料，然後每一個行就代表一個特徵值，請注意矩陣的第一行都是 1，這是我們演算法自己補上的 $x_0$，並不是原本訓練資料就會有的特徵值，以這個 Demo 資料來說，每筆資料只有 4 個特徵值（feature），像第一筆資料的 4 個特徵值就是 0.97681 0.10723 0.64385 0.29556，演算法將前面補上 $x_0 = 1$，就變成了現在看到的樣子。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.  1.  1.  1.  1.  1. -1.  1. -1. -1.  1.  1.  1. -1. -1.  1.  1.  1.\n",
      " -1.  1.  1.  1.  1.  1.  1.  1. -1.  1.  1. -1. -1.  1.  1. -1.  1.  1.\n",
      " -1. -1.  1. -1. -1.  1. -1.  1.  1.  1. -1. -1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1. -1. -1.  1. -1.  1. -1. -1.  1. -1.  1. -1. -1.  1.  1.  1.\n",
      " -1.  1.  1.  1.  1.  1.  1. -1.  1.  1.  1. -1.  1.  1. -1.  1.  1.  1.\n",
      "  1.  1.  1.  1. -1.  1. -1.  1.  1. -1.  1.  1.  1.  1. -1.  1.  1.  1.\n",
      "  1. -1.  1. -1.  1.  1. -1.  1.  1.  1.  1. -1.  1. -1. -1. -1.  1.  1.\n",
      "  1.  1.  1.  1.  1. -1. -1.  1.  1. -1.  1. -1.  1.  1.  1. -1.  1. -1.\n",
      " -1.  1. -1. -1.  1.  1.  1.  1. -1.  1.  1.  1.  1.  1.  1.  1.  1. -1.\n",
      " -1. -1.  1. -1.  1. -1.  1. -1.  1.  1. -1. -1.  1. -1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1. -1.  1.  1. -1.  1.  1.  1.  1.  1. -1.  1.  1.  1.  1.\n",
      "  1.  1. -1. -1. -1. -1.  1. -1.  1.  1. -1.  1. -1. -1.  1.  1.  1.  1.\n",
      "  1.  1.  1. -1.  1. -1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1. -1.\n",
      "  1. -1.  1.  1. -1.  1.  1.  1.  1. -1.  1. -1.  1.  1.  1.  1.  1. -1.\n",
      "  1. -1.  1.  1.  1. -1. -1.  1.  1.  1.  1.  1. -1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1. -1.  1.  1.  1.  1. -1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1. -1.  1. -1.  1.  1.  1. -1. -1. -1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1. -1.  1.  1.  1. -1.  1.  1. -1. -1. -1.  1.  1. -1.\n",
      " -1.  1. -1. -1. -1.  1.  1.  1.  1.  1.  1.  1.  1. -1.  1.  1.  1.  1.\n",
      " -1.  1.  1. -1. -1.  1. -1.  1.  1. -1.  1.  1.  1.  1.  1. -1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1. -1.  1.  1.  1. -1.  1. -1.  1.  1.  1. -1.  1.\n",
      "  1.  1. -1. -1.  1.  1.  1.  1.  1.  1.  1.  1.]\n"
     ]
    }
   ],
   "source": [
    "print(pla_bc.train_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然後訓練資料的答案就存在 `train_Y` 中，也就代表每筆訓練資料的答案是什麼，正分類就是 1，負分類就是 -1。\n",
    "\n",
    "接下來讓我們進行下一步，設定參數："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('naive_cycle', 1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pla_bc.set_param(loop_mode='naive_cycle', step_alpha=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PLA 這個演算法我只提供兩個參數可以調，一個是 `loop_mode`，用來調整 PLA 選擇訓練資料來檢查自己猜錯或猜對的選法，預設是使用 `naive_cycle` ，會照著訓練資料的順序一個一個檢測，有錯就修正 w。你也可以設成使用 `random`，這樣 PLA 檢測時就會隨便選擇一個點來檢測，有錯就修正 w。\n",
    "\n",
    "另一個參數是 `step_alpha`，用來調整 PLA 每次有錯就修正 w 要修正多少量，原則上設成 1 就可以了。\n",
    "\n",
    "接下來就可以再進行下一步，初始化："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0.,  0.,  0.])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pla_bc.init_W()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "初始化時，我們可以得到一個最初的權重值 w，通常就是個 0 向量了，但有時我們可以用 Linear Regression 來初始化，加速演算法，之後我們會再介紹，一樣我們將初始化的 w 印出來看看："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "print(pla_bc.W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "好！果然是 0 向量，一切準備就緒，接下來就是重頭戲「訓練」了："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-3.       ,  3.0841436, -1.583081 ,  2.391305 ,  4.5287635])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pla_bc.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "登登登！訓練完成，我們會得到一個全新的權重值 w，根據 PLA 的運算，這個 w 可以將資料完全分類正確！這就是機器學習神奇的地方！\n",
    "\n",
    "我們一樣把 PLA 計算出來的 w 印出來看看："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-3.         3.0841436 -1.583081   2.391305   4.5287635]\n"
     ]
    }
   ],
   "source": [
    "print(pla_bc.W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "果然不是個 0 向量了呢！\n",
    "\n",
    "有了這個 w，我們就可以用它來預測未來的資料，讓我拿一筆測試資料 0.97959 0.40402 0.96303 0.28133 1 來預測看看，前面 4 個值是這筆測試資料的特徵值，後面的 1 代表這筆測試資料的答案，我們來看看預測結果："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data = '0.97959 0.40402 0.96303 0.28133 1'\n",
    "prediction = pla_bc.prediction(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "將預測結果印出來看看："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prediction': 1.0, 'input_data_x': array([ 1.     ,  0.97959,  0.40402,  0.96303,  0.28133]), 'input_data_y': 1.0}\n"
     ]
    }
   ],
   "source": [
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prediction 這個方法會把預測結果回傳成一個 dictionary，預測結果的 key 是 prediction，value 是 1，測試資料的答案也是 1，所以 PLA 正確預測了結果！\n",
    "\n",
    "假設我們現在要預測的是未知的資料、一些我們還沒有分好類的資料，那我們就是把資料特徵值向量丟進去 prediction 方法，並設定 ｀`mode='future_data'`，代表是做未知資料的預測，就可以進行預測了，比如丟進去 0.29634 0.4012 0.40266 0.67864 這筆特徵資料試試看："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "future_data = '0.29634 0.4012 0.40266 0.67864'\n",
    "prediction = pla_bc.prediction(future_data, mode='future_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "將預測結果印出來看看："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prediction': 1.0, 'input_data_x': array([ 1.     ,  0.29634,  0.4012 ,  0.40266,  0.67864]), 'input_data_y': None}\n"
     ]
    }
   ],
   "source": [
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PLA 會忠實的觀察資料給出答案，它認為這筆資料的答案也是 1。（事實上真的是）\n",
    "\n",
    "當然，如果只是看一、兩筆資料猜對，大家可能會認為這只是運氣好，所以我們必須計算 PLA 在整個訓練資料集及整個測試資料集的預測表現如何。我們提供了很簡易的方法可以計算整體的錯誤率，如果要看 PLA 在整個訓練資料集的預測錯誤率（$E_{in}$）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(pla_bc.calculate_avg_error(pla_bc.train_X, pla_bc.train_Y, pla_bc.W))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PLA 在訓練資料的預測錯誤率是完美的 0！這是當然的，因為 PLA 在線性可分的資料裡，一定會調整到沒有錯誤為止。\n",
    "\n",
    "現在我們來看看 PLA 在整個測試資料集的預測錯誤率（$E_{out}$），在此之前，我們必須先載入測試資料集，一樣 FukuML 有提供 Demo 版本的測試資料集:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 1.      ,  0.97959 ,  0.40402 ,  0.96303 ,  0.28133 ],\n",
       "        [ 1.      ,  0.29634 ,  0.4012  ,  0.40266 ,  0.67864 ],\n",
       "        [ 1.      ,  0.34922 ,  0.99751 ,  0.23234 ,  0.52115 ],\n",
       "        [ 1.      ,  0.65637 ,  0.7181  ,  0.72843 ,  0.93113 ],\n",
       "        [ 1.      ,  0.079695,  0.57218 ,  0.70591 ,  0.33812 ],\n",
       "        [ 1.      ,  0.71206 ,  0.51569 ,  0.18168 ,  0.5557  ],\n",
       "        [ 1.      ,  0.17528 ,  0.2625  ,  0.8306  ,  0.029669],\n",
       "        [ 1.      ,  0.93895 ,  0.93941 ,  0.72496 ,  0.95655 ],\n",
       "        [ 1.      ,  0.046136,  0.94413 ,  0.038311,  0.26812 ],\n",
       "        [ 1.      ,  0.072491,  0.2242  ,  0.62592 ,  0.67238 ]]),\n",
       " array([ 1.,  1., -1.,  1., -1.,  1., -1.,  1., -1.,  1.]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pla_bc.load_test_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "載入測試資料之後，我們就可以計算 PLA 在測試資料集的預測錯誤率（$E_{out}$）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(pla_bc.calculate_test_data_avg_error())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PLA 在測試資料的預測錯誤率也是完美的 0，當然這某種程度是因為我們的 Demo 資料有設計過，不過理論上測試資料的預測錯誤率應該不會和訓練資料的預測錯誤率差太多，只要實驗過程是一個客觀的過程、沒有經過人為的污染，機器學習的演算法的確可以做到正確的預測。\n",
    "\n",
    "以上，你大概已經學會使用 FukuML 提供的 PLA 做訓練，然後使用訓練完成的 w 來進行未知資料的預測了，真的五個步驟就可以做完了！很簡單吧！"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
